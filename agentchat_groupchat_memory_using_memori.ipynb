{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c50176b",
   "metadata": {},
   "source": [
    "# Multi-agent Conversation and Recall using Memori\n",
    "\n",
    "In this notebook, you'll learn how to create AI agents that can **remember** conversations and use that memory in future discussions. We'll build a simple consulting team that gets smarter over time.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to set up AI agents that work together\n",
    "- How to give agents persistent memory\n",
    "- How memory makes conversations better and more helpful\n",
    "\n",
    "## The scenario:\n",
    "We'll create a **Software Development Consulting Team** with two AI agents:\n",
    "- **Alex** - Technical Architect (designs systems)\n",
    "- **Sam** - Full-Stack Developer (builds applications)\n",
    "\n",
    "They'll help a client build an e-commerce website, and they'll remember everything discussed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db60137",
   "metadata": {},
   "source": [
    "## What is Memori?\n",
    "\n",
    "**Memori** is an open-source memory engine that provides persistent, intelligent memory for any LLM using standard SQL databases. Memori uses multiple agents working together to intelligently promote essential long-term memories to short-term storage for faster context injection.\n",
    "\n",
    "With a single line of code `memori.enable()` any LLM gains the ability to remember conversations, learn from interactions, and maintain context across sessions. The entire memory system is stored in a standard SQLite database (or PostgreSQL/MySQL for enterprise deployments), making it fully portable, auditable, and owned by the user.\n",
    "\n",
    "### Key features:\n",
    "- **Auto-recording**: Automatically saves all conversations\n",
    "- **Works with existing databases**: SQLite, PostgreSQL, MySQL, MongoDB\n",
    "- **Smart memory**: AI decides what's important to remember\n",
    "- **Cross-session**: Agents remember between different conversations\n",
    "- **Zero setup**: Just initialize and enable - that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc076d",
   "metadata": {},
   "source": [
    "## Requirements \n",
    "\n",
    "Before we start, we need to install some packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6219f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memorisdk\n",
      "  Using cached memorisdk-2.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting autogen-agentchat\n",
      "  Using cached autogen_agentchat-0.7.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting autogen-ext[openai]\n",
      "  Using cached autogen_ext-0.7.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting loguru>=0.6.0 (from memorisdk)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pydantic>=2.0.0 (from memorisdk)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting sqlalchemy>=2.0.0 (from memorisdk)\n",
      "  Using cached sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting openai>=1.0.0 (from memorisdk)\n",
      "  Using cached openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting litellm>=1.0.0 (from memorisdk)\n",
      "  Using cached litellm-1.77.5-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting autogen-core==0.7.4 (from autogen-agentchat)\n",
      "  Using cached autogen_core-0.7.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonref~=1.1.0 (from autogen-core==0.7.4->autogen-agentchat)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-api>=1.34.1 (from autogen-core==0.7.4->autogen-agentchat)\n",
      "  Using cached opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pillow>=11.0.0 (from autogen-core==0.7.4->autogen-agentchat)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting protobuf~=5.29.3 (from autogen-core==0.7.4->autogen-agentchat)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting typing-extensions>=4.0.0 (from autogen-core==0.7.4->autogen-agentchat)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->memorisdk)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.0.0->memorisdk)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.0.0->memorisdk)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting aiofiles (from autogen-ext[openai])\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tiktoken>=0.8.0 (from autogen-ext[openai])\n",
      "  Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting click (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fastuuid>=0.13.0 (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached fastuuid-0.13.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting httpx>=0.23.0 (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.2 (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting tokenizers (from litellm>=1.0.0->memorisdk)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<4.0.0,>=3.1.2->litellm>=1.0.0->memorisdk)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm>=1.0.0->memorisdk)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting anyio (from httpx>=0.23.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx>=0.23.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.23.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm>=1.0.0->memorisdk)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.0.0->memorisdk)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.0.0->memorisdk)\n",
      "  Using cached jiter-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.0.0->memorisdk)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>=1.0.0->memorisdk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken>=0.8.0->autogen-ext[openai])\n",
      "  Using cached regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken>=0.8.0->autogen-ext[openai])\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai])\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai])\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers->litellm>=1.0.0->memorisdk)\n",
      "  Using cached huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting filelock (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.0.0->memorisdk)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.0.0->memorisdk)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.0.0->memorisdk) (25.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.0.0->memorisdk)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.0.0->memorisdk)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Using cached memorisdk-2.1.1-py3-none-any.whl (217 kB)\n",
      "Using cached autogen_agentchat-0.7.4-py3-none-any.whl (119 kB)\n",
      "Using cached autogen_core-0.7.4-py3-none-any.whl (101 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached autogen_ext-0.7.4-py3-none-any.whl (328 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached litellm-1.77.5-py3-none-any.whl (9.2 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached aiohttp-3.12.15-cp312-cp312-macosx_11_0_arm64.whl (469 kB)\n",
      "Using cached multidict-6.6.4-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached yarl-1.20.1-cp312-cp312-macosx_11_0_arm64.whl (89 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached fastuuid-0.13.5-cp312-cp312-macosx_11_0_arm64.whl (244 kB)\n",
      "Using cached frozenlist-1.7.0-cp312-cp312-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (316 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached propcache-0.3.2-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl (345 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (996 kB)\n",
      "Using cached regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl (287 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: zipp, urllib3, typing-extensions, tqdm, sniffio, rpds-py, regex, pyyaml, python-dotenv, protobuf, propcache, pillow, multidict, MarkupSafe, loguru, jsonref, jiter, idna, hf-xet, h11, fsspec, frozenlist, filelock, fastuuid, distro, click, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, aiofiles, yarl, typing-inspection, sqlalchemy, requests, referencing, pydantic-core, jinja2, importlib-metadata, httpcore, anyio, aiosignal, tiktoken, pydantic, opentelemetry-api, jsonschema-specifications, huggingface-hub, httpx, aiohttp, tokenizers, openai, jsonschema, autogen-core, litellm, autogen-ext, autogen-agentchat, memorisdk\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58/58\u001b[0m [memorisdk]emorisdk]utogen-ext]hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 aiofiles-24.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.3.0 autogen-agentchat-0.7.4 autogen-core-0.7.4 autogen-ext-0.7.4 certifi-2025.8.3 charset_normalizer-3.4.3 click-8.3.0 distro-1.9.0 fastuuid-0.13.5 filelock-3.19.1 frozenlist-1.7.0 fsspec-2025.9.0 h11-0.16.0 hf-xet-1.1.10 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.35.1 idna-3.10 importlib-metadata-8.7.0 jinja2-3.1.6 jiter-0.11.0 jsonref-1.1.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 litellm-1.77.5 loguru-0.7.3 memorisdk-2.1.1 multidict-6.6.4 openai-1.109.1 opentelemetry-api-1.37.0 pillow-11.3.0 propcache-0.3.2 protobuf-5.29.5 pydantic-2.11.9 pydantic-core-2.33.2 python-dotenv-1.1.1 pyyaml-6.0.3 referencing-0.36.2 regex-2025.9.18 requests-2.32.5 rpds-py-0.27.1 sniffio-1.3.1 sqlalchemy-2.0.43 tiktoken-0.11.0 tokenizers-0.22.1 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0 yarl-1.20.1 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell first!\n",
    "\n",
    "!pip install memorisdk autogen-agentchat \"autogen-ext[openai]\" python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008e3f5",
   "metadata": {},
   "source": [
    "### Set up your OpenAI API key \n",
    "\n",
    "You'll need an OpenAI API key to run this example. You can:\n",
    "1. Set it as an environment variable: `export OPENAI_API_KEY=\"sk-your-key-here\"`\n",
    "2. Create a `.env` file with: `OPENAI_API_KEY=sk-your-key-here`\n",
    "3. Or uncomment and fill in the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7961a",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries \n",
    "\n",
    "Let's import everything we need for our multi-agent conversation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6a226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "# AutoGen imports - for creating AI agent teams\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Memori import - for giving agents memory\n",
    "from memori import Memori\n",
    "\n",
    "# For loading environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d06aaa",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Memory System \n",
    "\n",
    "This is the magic step! We create a memory system that will automatically record and remember all conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517941c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 19:02:45.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmemori.core.memory\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m171\u001b[0m - \u001b[1mUsing default OpenAI provider (no specific provider configured)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory system initialized!\n",
      "Database: consulting_memory.db\n",
      "Auto-recording enabled - all conversations will be remembered!\n"
     ]
    }
   ],
   "source": [
    "# Create the memory system - this is where all conversations will be saved\n",
    "memory = Memori(\n",
    "    database_connect=\"sqlite:///consulting_memory.db\",  # Local database file\n",
    "    auto_ingest=True,        # Automatically save all conversations\n",
    "    conscious_ingest=True,   # AI decides what's important to remember\n",
    "    verbose=False,           # Set to True to see what's happening behind the scenes\n",
    "    namespace=\"consulting\"   # Separate memory space for this project\n",
    ")\n",
    "\n",
    "# Enable the memory system\n",
    "memory.enable()\n",
    "\n",
    "print(\"Memory system initialized!\")\n",
    "print(\"Database: consulting_memory.db\")\n",
    "print(\"Auto-recording enabled - all conversations will be remembered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda986b",
   "metadata": {},
   "source": [
    "## Step 3: Create AI Agents \n",
    "\n",
    "Now let's create our consulting team! We'll make two AI agents with different expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aec9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex (Technical Architect) created\n",
      "Sam (Full-Stack Developer) created\n",
      "Both agents have persistent memory enabled!\n"
     ]
    }
   ],
   "source": [
    "# Set up the AI model (OpenAI GPT-4o-mini)\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create Alex - Technical Architect\n",
    "alex = AssistantAgent(\n",
    "    name=\"Alex\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are Alex, a Senior Technical Architect.\n",
    "    \n",
    "    You have persistent memory and remember:\n",
    "    - Client requirements and constraints\n",
    "    - Technical decisions made in past conversations\n",
    "    - Budget and timeline discussions\n",
    "    \n",
    "    Always reference previous conversations when relevant.\n",
    "    Keep your responses focused and practical.\"\"\",\n",
    ")\n",
    "\n",
    "# Create Sam - Full-Stack Developer\n",
    "sam = AssistantAgent(\n",
    "    name=\"Sam\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"You are Sam, a Senior Full-Stack Developer.\n",
    "    \n",
    "    You have persistent memory and remember:\n",
    "    - Client's technical preferences and team skills\n",
    "    - Implementation decisions from past discussions\n",
    "    - Development approaches we've recommended\n",
    "    \n",
    "    Build upon previous conversations and maintain consistency.\n",
    "    Focus on practical implementation advice.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"Alex (Technical Architect) created\")\n",
    "print(\"Sam (Full-Stack Developer) created\")\n",
    "print(\"Both agents have persistent memory enabled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30add121",
   "metadata": {},
   "source": [
    "## Step 4: Create the Team\n",
    "\n",
    "Let's put our agents together in a team that can collaborate on client problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed34903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulting team created!\n",
      "Team members: Alex (Architect) + Sam (Developer)\n",
      "They'll take turns responding to client questions\n"
     ]
    }
   ],
   "source": [
    "# Create a team where agents take turns (round-robin)\n",
    "consulting_team = RoundRobinGroupChat(\n",
    "    participants=[alex, sam],  # Our two agents\n",
    "    termination_condition=MaxMessageTermination(max_messages=6)  # Stop after 6 messages\n",
    ")\n",
    "\n",
    "print(\"Consulting team created!\")\n",
    "print(\"Team members: Alex (Architect) + Sam (Developer)\")\n",
    "print(\"They'll take turns responding to client questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72ddeb",
   "metadata": {},
   "source": [
    "## Step 5: First Consultation - Setting Requirements üìù\n",
    "\n",
    "Let's simulate our first client meeting where they share their project requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99202fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIENT REQUEST 1: Initial Requirements\n",
      "==================================================\n",
      "\n",
      "Hi team! I'm Sarah, and I'm building a new e-commerce platform for my retail business.\n",
      "\n",
      "Here are my requirements:\n",
      "- Need to handle 10,000+ products\n",
      "- Process payments securely\n",
      "- Manage inventory in real-time\n",
      "- My budget is $50,000\n",
      "- My team knows React and Python well\n",
      "- We prefer modern, maintainable technology\n",
      "\n",
      "What architecture would you recommend?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Team Response:\n",
      "\n",
      "1. user: \n",
      "Hi team! I'm Sarah, and I'm building a new e-commerce platform for my retail business.\n",
      "\n",
      "Here are my requirements:\n",
      "- Need to handle 10,000+ products\n",
      "- Process payments securely\n",
      "- Manage inventory in real-time\n",
      "- My budget is $50,000\n",
      "- My team knows React and Python well\n",
      "- We prefer modern, maintainab...\n",
      "\n",
      "2. Alex: Hi Sarah! Based on the requirements you've outlined and considering your team's expertise with React and Python, I recommend the following architecture for your e-commerce platform:\n",
      "\n",
      "### Architecture Overview\n",
      "\n",
      "1. **Frontend**:\n",
      "   - **React**: Utilize React for building a dynamic and responsive user ...\n",
      "\n",
      "3. Sam: Hi Sarah! It‚Äôs great to see your e-commerce platform taking shape. Building on Alex's recommendations, I'd like to provide a bit more detail and some practical implementation steps for the architecture he outlined.\n",
      "\n",
      "### Implementation Steps\n",
      "\n",
      "1. **Frontend Development**:\n",
      "   - Set up your React enviro...\n",
      "\n",
      "4. Alex: Hi Sam! Those are excellent recommendations and practical steps to further detail the implementation of Sarah‚Äôs e-commerce platform. \n",
      "\n",
      "To add on to your points, here are a few considerations to keep in mind as Sarah and her team move forward:\n",
      "\n",
      "### Additional Considerations\n",
      "\n",
      "1. **Security**:\n",
      "   - Ens...\n",
      "\n",
      "5. Sam: Hi Alex! Those are great additional considerations that will definitely strengthen Sarah's project. I‚Äôd like to expand on a couple of your points and provide further context that could be useful for Sarah and her team as they move forward.\n",
      "\n",
      "### Expanded Considerations\n",
      "\n",
      "1. **Security**:\n",
      "   - **HTTPS*...\n",
      "\n",
      "6. Alex: Hi Sam! Those expanded considerations are very insightful and provide significant value as Sarah and her team embark on this project. \n",
      "\n",
      "I‚Äôd like to emphasize a few areas and add some more practical strategies that can support their e-commerce platform further:\n",
      "\n",
      "### Highlights and Additional Strategi...\n"
     ]
    }
   ],
   "source": [
    "# First client conversation - gathering requirements\n",
    "client_request_1 = \"\"\"\n",
    "Hi team! I'm Sarah, and I'm building a new e-commerce platform for my retail business.\n",
    "\n",
    "Here are my requirements:\n",
    "- Need to handle 10,000+ products\n",
    "- Process payments securely\n",
    "- Manage inventory in real-time\n",
    "- My budget is $50,000\n",
    "- My team knows React and Python well\n",
    "- We prefer modern, maintainable technology\n",
    "\n",
    "What architecture would you recommend?\n",
    "\"\"\"\n",
    "\n",
    "print(\"CLIENT REQUEST 1: Initial Requirements\")\n",
    "print(\"=\" * 50)\n",
    "print(client_request_1)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTeam Response:\")\n",
    "\n",
    "# Run the team conversation\n",
    "result_1 = await consulting_team.run(task=client_request_1)\n",
    "\n",
    "# Show the team's response\n",
    "for i, message in enumerate(result_1.messages, 1):\n",
    "    print(f\"\\n{i}. {message.source}: {message.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f16b9",
   "metadata": {},
   "source": [
    "## Step 6: Follow-up Consultation - Database Decision\n",
    "\n",
    "Now let's see the magic of memory! The client asks a follow-up question, and our agents should remember the previous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8355e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIENT REQUEST 2: Database & Inventory (Notice: References previous meeting!)\n",
      "==================================================\n",
      "\n",
      "Great recommendations from our last meeting! \n",
      "\n",
      "Now I'm concerned about the database choice. Given our product catalog size \n",
      "and the budget constraints we discussed, what specific database solution \n",
      "would work best for our e-commerce platform?\n",
      "\n",
      "Also, how should we handle the inventory tracking?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Team Response (with memory of previous conversation):\n",
      "\n",
      "1. user: \n",
      "Great recommendations from our last meeting! \n",
      "\n",
      "Now I'm concerned about the database choice. Given our product catalog size \n",
      "and the budget constraints we discussed, what specific database solution \n",
      "would work best for our e-commerce platform?\n",
      "\n",
      "Also, how should we handle the inventory tracking?\n",
      "...\n",
      "\n",
      "2. Sam: Hi Sarah! I'm happy to provide additional insights on your database choice and inventory tracking strategies, especially considering your product catalog size and budget constraints.\n",
      "\n",
      "### Database Choice\n",
      "\n",
      "Since you‚Äôre handling 10,000+ products and looking for a reliable solution within your budget, ...\n",
      "\n",
      "3. Alex: Hi Sarah! I appreciate your follow-up on the database choice and inventory management. To build on Sam‚Äôs insights, let‚Äôs discuss a few additional elements that can enhance your approach.\n",
      "\n",
      "### Database Choice\n",
      "\n",
      "As previously mentioned, **PostgreSQL** is indeed an excellent choice for your e-commerce p...\n",
      "\n",
      "4. Sam: Hi Alex! Those additional suggestions are really valuable and will definitely enhance Sarah‚Äôs approach to both the database selection and inventory tracking strategies. \n",
      "\n",
      "### Recap and Final Considerations\n",
      "\n",
      "1. **Database Choice**:\n",
      "    - **PostgreSQL** remains a strong candidate for its relational ca...\n",
      "\n",
      "5. Alex: Hi Sam! Your recap and final considerations encapsulate our discussion perfectly. It‚Äôs clear that you‚Äôre providing Sarah with the right guidance to make informed decisions for her e-commerce platform. Here are a few additional thoughts to consider moving forward:\n",
      "\n",
      "### Implementation Steps\n",
      "\n",
      "1. **Prot...\n",
      "\n",
      "6. Sam: Hi Alex! I appreciate your thoughtful addition to the implementation steps and long-term considerations for Sarah‚Äôs e-commerce platform. Each of your points significantly enhances the overall strategy. \n",
      "\n",
      "### Key Takeaways for Sarah‚Äôs E-Commerce Project\n",
      "\n",
      "1. **Prototyping**:\n",
      "    - Using wireframing to...\n"
     ]
    }
   ],
   "source": [
    "# Second client conversation - building on previous discussion\n",
    "client_request_2 = \"\"\"\n",
    "Great recommendations from our last meeting! \n",
    "\n",
    "Now I'm concerned about the database choice. Given our product catalog size \n",
    "and the budget constraints we discussed, what specific database solution \n",
    "would work best for our e-commerce platform?\n",
    "\n",
    "Also, how should we handle the inventory tracking?\n",
    "\"\"\"\n",
    "\n",
    "print(\"CLIENT REQUEST 2: Database & Inventory (Notice: References previous meeting!)\")\n",
    "print(\"=\" * 50)\n",
    "print(client_request_2)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTeam Response (with memory of previous conversation):\")\n",
    "\n",
    "# Run the team conversation - they should remember the $50K budget and 10K+ products\n",
    "result_2 = await consulting_team.run(task=client_request_2)\n",
    "\n",
    "# Show the team's response\n",
    "for i, message in enumerate(result_2.messages, 1):\n",
    "    print(f\"\\n{i}. {message.source}: {message.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d3c83",
   "metadata": {},
   "source": [
    "## Step 7: Third Consultation - Development Approach üõ†Ô∏è\n",
    "\n",
    "Let's test the memory even more! The client asks about development approach, referencing team size and timeline that weren't explicitly mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third client conversation - development strategy\n",
    "client_request_3 = \"\"\"\n",
    "Perfect! The database recommendations make sense.\n",
    "\n",
    "Now for the development approach - should we build this as a monolith first \n",
    "or go straight to microservices? \n",
    "\n",
    "Remember, we have a small team (just 3 developers) and need to launch in 6 months.\n",
    "Also, keep in mind our React and Python skills that I mentioned earlier.\n",
    "\"\"\"\n",
    "\n",
    "print(\"CLIENT REQUEST 3: Development Approach (References team skills from first meeting!)\")\n",
    "print(\"=\" * 50)\n",
    "print(client_request_3)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTeam Response (should remember React/Python skills + budget):\")\n",
    "\n",
    "# Run the team conversation - they should remember all previous context\n",
    "result_3 = await consulting_team.run(task=client_request_3)\n",
    "\n",
    "# Show the team's response\n",
    "for i, message in enumerate(result_3.messages, 1):\n",
    "    print(f\"\\n{i}. {message.source}: {message.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cc270",
   "metadata": {},
   "source": [
    "## Step 8: Check What's in Memory \n",
    "\n",
    "Let's peek behind the scenes and see what our memory system has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the memory system has learned\n",
    "print(\"MEMORY SYSTEM ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Get memory statistics\n",
    "    stats = memory.get_memory_stats()\n",
    "    \n",
    "    print(f\"Total conversations recorded: {stats.get('total_conversations', 0)}\")\n",
    "    print(f\"Total memories stored: {stats.get('total_memories', 0)}\")\n",
    "    print(f\"Database location: consulting_memory.db\")\n",
    "    print(f\"Namespace: consulting\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Memory stats not available: {e}\")\n",
    "\n",
    "print(\"\\nAll conversations have been automatically saved!\")\n",
    "print(\"If you restart this notebook and run the agents again,\")\n",
    "print(\"   they will remember everything from today's conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52bf8",
   "metadata": {},
   "source": [
    "## Step 9: Test Memory Persistence\n",
    "\n",
    "Let's test if our agents truly remember by asking them directly what they learned about the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory recall\n",
    "memory_test = \"\"\"\n",
    "Hey team, I want to make sure we're all on the same page.\n",
    "\n",
    "Can you remind me of my key project requirements and the decisions \n",
    "we've made so far? I want to make sure nothing was missed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"MEMORY TEST: What do you remember about our project?\")\n",
    "print(\"=\" * 50)\n",
    "print(memory_test)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTeam's Memory Recall:\")\n",
    "\n",
    "# Run the memory test\n",
    "result_test = await consulting_team.run(task=memory_test)\n",
    "\n",
    "# Show what they remember\n",
    "for i, message in enumerate(result_test.messages, 1):\n",
    "    print(f\"\\n{i}. {message.source}: {message.content[:400]}...\")\n",
    "\n",
    "print(\"\\nAmazing! The agents remembered the key details from all our conversations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057b5e5",
   "metadata": {},
   "source": [
    "## Congratulations! You've Built Memory-Enhanced AI Agents!\n",
    "\n",
    "### What you accomplished:\n",
    "‚úÖ Created AI agents that work together as a team  \n",
    "‚úÖ Gave them persistent memory using Memori  \n",
    "‚úÖ Ran multiple conversations that build on each other  \n",
    "‚úÖ Saw how memory makes conversations more helpful  \n",
    "\n",
    "### Key insights from this demo:\n",
    "1. **Memory makes agents smarter**: They remembered budget ($50K), team skills (React/Python), and project constraints\n",
    "2. **Conversations build naturally**: Each discussion referenced previous context\n",
    "3. **Zero manual work**: Memori automatically captured and used relevant information\n",
    "4. **Persistent across sessions**: Restart the notebook and the agents will still remember!\n",
    "\n",
    "### Real-world applications:\n",
    "- **Customer Support**: Remember customer history and preferences\n",
    "- **Project Management**: Track decisions, requirements, and progress\n",
    "- **Personal Assistant**: Remember your preferences and past conversations\n",
    "- **Educational Tutoring**: Track student progress and learning style\n",
    "- **Medical Consultation**: Remember patient history and treatment plans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
